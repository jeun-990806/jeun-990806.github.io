---
title: FastFlow&#58; Accelerating Deep Learning Model Training with Smart Offloading of Input Data Pipeline
author: Jieun
date: 2023-06-15
layout: post
category: paper_reviews
---

* 요약: TensorFlow input pipeline 상의 bottleneck 중 하나인 preprocessing stall을 offloading을 통해 해결하여 딥러닝 워크로드 성능 향상

---

## Introduction
 * Large-scale DL model 학습에는 수백-수십억의 파라미터가 존재
 * 강력한 GPU의 등장으로 가능해짐 + 속도도 빨라짐
 * 그렇지만, GPU가 항상 빠른 학습을 보장하진 않음
 * 왜냐면 CPU가 병목 현상을 발생시킴 (Input pipeline에는 여러 단계가 있는데, 그중 디코딩, 전처리, augmenting은 CPU에서 처리함)
 * 두 자원을 동시에 사용하도록 overlap도 하지만, 전처리에 걸리는 시간이 GPU에서 학습 수행하는 시간보다 더 길어질 수 있음 (이를 Prep stall이라고 명명)

**전처리가 왜 중요할까?** AI 패러다임이 model-centric (괜찮은 모델이 중요)에서 data-centric (양질의 데이터가 중요)으로 넘어가면서, 전처리가 학습 결과를 좌우함


## Background and Motivation
 * 딥러닝 모델 학습 단계는 세 부분으로 나뉨
    1. Offline input preprocessing: raw data를 학습용 데이터셋으로 변환. 대량의 데이터 처리에는 분산 데이터 처리 시스템(e.g. Apache Spark, Flink)도 이용된다. Offline input preprocessing 오버헤드를 줄이기 위해 전처리된 데이터를 공유 스토리지 or 로컬 SSD에 저장하기도.
    2. Online input preprocessing: 학습용 데이터셋을 스토리지에 로드한 뒤, 디코딩 및 증강하여 텐서를 생성하고, 이를 GPU에게 넘겨 주어진 텐서로 gradient computation을 수행하도록 한다. 오프라인 전처리와 다르게, 한번 전처리가 끝난 결과물을 캐싱해뒀다가 쓸 수 없을 때도 있는데, 왜냐하면 1) 메모리 크기가 로드해야 하는 데이터 크기보다 작을 수 있고, 2)  증강은 데이터를 '랜덤'하게 변화시키는 것이 포인트라 재사용을 하면 안 됨
    3. Gradient computation: Forward + backward computation을 통해 모델의 weight update + loss 계산 + weight 재조정하여 모델의 loss를 줄임
    * 이 중에서 다루고자 하는 건 2번인데, 2번은 CPU-intensive operation이며 고사양 GPU의 gradient computation 속도를 따라잡지 못해 병목 현상을 발생시킨다. 이것을 Preprocessing stall, 줄여서 Prep stall이라고 부르겠다.
 * DL용으로 제공되는 클라우드 리소스들은 고정된 CPU:GPU 비율을 가지고 있는데, 이또한 Prep stall을 완전히 예방하진 못한다. 그렇다고 CPU가 너무 많아지면 Underutilized → 비효율적이 될 때도...
 * Prep stall에 관련된 연구들도 많지만 한계점도 존재
    1. Auto-tuning on local node
    2. Offloading to specialized hardware
    3. Offloading to remote CPUs
 * 본 연구에서 한 것은 input pipeline의 automatic offloading (remote CPUs로). decision을 위해 1) 언제 오프로딩할 것인지 2) 어떤 연산을 오프로딩할 것인지 3) 얼마나 많은 데이터를 오프로딩할 것인지 설정했다.

## System Overview
    (1) smart offloading을 위해 input pipelines, models, remote resource environment와 같은 정보를 수집해야 함. keras.Model에 정의된 model 인터페이스를 그대로 활용. 사용자는 프로파일링 구성과 deep-copy method를 설정할 수 있음.
    (2) 트레이닝 코드를 작성하고 나면, FastFlow가 해당 input pipeline과 학습 모델이 처음으로 실행되는 것인지 확인. 만약 처음 실행되는 것이라면 실제 학습 전 성능 메트릭을 프로파일링함
    (3) 프로파일링을 위해 로컬 노드에서 몇 회의 이터레이션을 수행. 프로파일링한 input pipeline, model, configuration과 같이 메트릭 스토리지에 프로파일링 결과를 저장. 다음에 같은 구성으로 실행될 때는 프로파일링을 건너뛰고 저장된 메트릭을 바로 불러옴. 
    (4) 메트릭을 기반으로 Prep stall이 존재하는지 파악하고 존재하는 경우 input pipeline을 수정하여 offloading함
    (5) 실제 offloading 및 학습 수행 시 해당 프레임워크가 지원하는 기능을 활용. 예를 들어 TF는 tf.data.service가 input pipeline의 remote CPUs로의 horizontal scaling을 지원하고 있음. *pipeline operator를 복사하고 사본 operators를 워커에서 실행시킴. 각 워커는 같은 TF 코드베이스에 존재. distributed operator가 원격 CPU에서 전처리된 데이터를 GPU로 fetch
 * FastFlow는 1) tf.data.service 워커 프로세스 생성 2) distributed operator에 conditional operator를 injection (conditional operator는 local-remote ratio를 파라미터로 받음) 3) 그 다음 모든 remote worker에 데이터를 고르게 분배 (각 worker는 CPU 자원의 양도 동일하고, homogeneous하다고 가정)
 * 주의할 점은 전처리할 데이터가 **공유 스토리지에 저장되어 있거나, 미리 워커에 다 복사되어 있어야 함**. 워커가 처리할 input data를 동적으로 변화시키기 위해서
 * 데이터를 중복하여 전처리하는 일이 없도록 전처리 과정 동기화가 필요. 이는 TF에서 제공하는 IndexProvider로 가능. 워커가 인덱스를 요청하면 요청 시마다 인덱스를 증감하여 되돌려주는데, 따라서 워커는 자기가 받아온 인덱스의 데이터를 전처리하기만 하면 중복이 없음!

## Smart Offloading Policy
 * 다섯 가지 메트릭, Gthp, Lthp, Rthp, Ocycle, Pcycle 활용
    1. Prep stall detection: Lthp = Gthp는 stall이 없는 상태이므로 오프로딩하지 않음. Gthp가 Lthp보다 더 크고, 그것이 SpeedUpThreshold 이상인 경우 오프로딩 (너무 작은 수준-보통 10% 미만의 속도 향상을 위해 오프로딩하지는 않음)
    2. Offloading operator selection: Input pipeline candidate가 세 개 있음. (1) Prep만 오프로딩: 로컬→원격으로 데이터 보내는 속도 > 원격에서의 데이터 Read 속도일 때 효율적 (2) Read-Prep만 오프로딩: 다수의 batch prefetch시 발생하는 threading overhead가 적음 (3) Read-Prep-Batch까지 오프로딩: granularity가 batch 단위가 되어 network I/O 및 encoding/decoding overhead가 줄어듦
    3. Offloading data ratio decision: Upper(이 이상 오프로딩할 시 원격 CPU/network I/O가 bottleneck이 됨, Rthp)와 Lower(이 이하로 오프로딩할 시 로컬 CPU가 bottleneck이 됨) 설정. Lower는 Gthp-Lthp에, 오프로딩하여 전처리된 데이터를 인코딩/디코딩하는 데 소비되는 사이클과 전처리에 소비되는 사이클의 비율을 보정하여 계산. 만약 Lower > Upper인 경우 (e.g. 원격 CPU의 성능 또는 네트워크 대역폭이 떨어지는 경우), 최선의 방법은 로컬 CPU와 원격 CPU에 CPU 자원 성능에 비례하여 태스크를 분배하는 것이다.

## Smart Offloading Mechanism
 * 메트릭 수집 단계에서는 전체 데이터를 다 처리하지 않음. (We empirically found that profiling 50-100 steps leads to high accuracy with small overheads)
 * 각 메트릭은 다음과 같이 수집함
    - Gthp: 전처리된 데이터를 모두 캐싱해두고 GPU에 feed
    - Lthp, Gthp: 각각의 Input pipeline을 모두 실행하면서 throughput 측정
    - Ocycle, Pcycle: gradient computation을 제외한 나머지 input pipeline을 실행하여 측정
 * 메트릭 저장 시에는, key-value store를 활용해 구성별로 테스트 결과(메트릭)를 저장해두고 이후에 재사용 (프로파일링 오버헤드 제거)
 * 노드에 골고루 오프로딩하기 위해 counter-based approach 채택. 카운터는 노드별로 존재하여 데이터를 읽거나 전처리할때마다 해당 노드의 카운터를 증감시킴.

## Implementation
 * TF 2.7.0에서 구현했고 tf.data.service의 기본적인 오프로딩 메커니즘을 활용
 * iterator operator를 수정하여 한 batch를 전처리할때마다 처리된 데이터의 크기와 걸린 시간을 측정하고 이를 프로파일러에게 넘겨주게 만듦. 프로파일러는 이 정보를 활용하여 throughput 계산
 * Gthp 계산을 위해, 전처리된 데이터를 모두 캐싱하는 방법은 다음과 같음: take(1).cache().repeat() -> 데이터셋에서 배치 하나를 가져와서 캐싱한 뒤, 그걸 반복적으로 사용
 * CPU 사이클 계산에는 psutil 이용

## Evaluation
 * 셋업은 쿠버네티스로 Private Cloud System(PCS)를 구축
 * GPU 컨테이너는 7x개의 vCPU와 1y개의 NVIDIA V100
 * CPU 컨테이너는 7z개의 vCPU
    - (1<=x<=2, 1<=y<=4, 1<=z<=4)
 * Local GPU Node: 기본적으로 7개의 vCPU와 1개의 GPU를 사용. 이는 자주 사용되는 구성(8:1)과 유사하기 때문
 * Remote CPU Node + High Network Bandwidth (>20Gbps)
 * Remote CPU Node + Low Network Bandwidth (<1Gbps)
 * Workloads는 총 7개
 * Baseline: TF-NO (TF + No Offloading), TF-DSR (TF + tf.data.service로 Remote Worker 활용. p'3), TF-DSRL (TF + tf.data.service로 Local Worker와 Remote Worker 활용. p'3), DALI (전처리를 Local GPU로 오프로딩. 모든 전처리 연산을 다 수행할 수 있는 것은 아니어서, 가능한 경우만 실험함)
 * Performance effect of offloading decisions: 
 * Profiling overhead: 상당히 적은 편! (특히 메트릭 재사용 시)
