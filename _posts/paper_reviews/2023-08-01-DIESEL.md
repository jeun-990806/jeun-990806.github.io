---
title: DIESEL&#58; A Dataset-Based Distributed Storage and Caching System for Large-Scale Deep Learning Training
author: Jieun
date: 2023-08-01
layout: post
category: paper_reviews
---

## INTRODUCTION
* Deep learning training (이하 DLT) 작업에 사용되는 데이터 세트 안에는 수많은 작은 파일들이 카테고리 별로 분류되어 있는데, 따라서 파일 접근에 걸리는 시간이 매우 큰 부분을 차지한다.
  - A popular image dataset for computer vision tasks, ImageNet-1K, contains more than 1.28 million files in 1,000 categories. (Average image size is 110KB)
  - The Open Images dataset contains around 9 million images. (Average image size is 60KB) 
* DL 프레임워크는 컴퓨터 클러스터에서 동작하며, 데이터 세트 저장에 공유 스토리지를 이용하기도 하는데, 공유 스토리지를 이용하는 컴퓨터 클러스터가 커지면 수많은 랜덤 액세스가 동시에 발생해 스토리지가 포화 상태가 될 수 있다.
* 작은 크기의 파일에 접근하는 것은 *메타데이터-인텐시브*하므로, 관련 연구들은 캐싱을 통해 메타데이터 서버의 성능 개선에 주안점을 두고 있지만, 대부분 파일 수준의 캐싱 시스템이기 때문에 느리다.
  - Global caching system, such as Quiver and in-memory caching middleware, e.g., Memcached cluster, are deployed to relieve the I/O pressure on the underlying storage systems.
* 이러한 I/O 문제는 가속기의 성능이 강력할수록 심화된다.
  - Massively parallel processors, the performance of data movements significantly falls behind the computation speed.
* **본 논문에서는, DLT 작업의 I/O 특성을 이용한 스토리지 및 캐싱 시스템 DIESEL을 설계했다.**
  - First, DLT tasks have seperate read and write phases. (This seperation enables *stateless metadata cache designs*.)
  - Second, in each training job, the dataset is read multiple times. (Dataset-based access pattern)
  - Third, in DLT tasks, the input files are not required to be accessed in a fixed order, as long as the access order is randomized. (We can perform the data shuffling in a *storage-friendly layout* with very small memory footprint.)
* DIESEL의 특징은 다음과 같다.
  - **Metadata processing offloading**: 여러 파일을 청크 단위(e.g., 4MB)로 압축하여 in-memory key-value store에 저장하고, 학습 시 메타데이터 스냅샷을 DLT 노드에 분산시킨다.
  - **Task-grained distributed caching system**: 모든 DLT 노드의 메모리를 합쳐서 사용한다. 한 노드에서 발생한 문제가 전체 작업에 미치는 영향을 최소화한다.
  - **Chunk-wise shuffling**: 청크 단위 I/O지만, 파일은 기존 방식처럼 완전히 랜덤한 순서로 읽는다.


## BACKGROUND
### I/O Charateristics of Deep Learning
* DLT 작업의 파일 액세스 특성은 다음과 같다.
  1. Separate write and read phases: 학습 단계에서는 파일을 읽기만 한다.
  2. Iterative and traversal read: 에포크 횟수만큼 데이터 세트를 반복해서 읽는다. 한 에포크에 전체 데이터 세트의 모든 파일을 한 번씩 읽는 것이 일반적이다.
  3. Shuffled file access order: 매 에포크 시작 전 파일 순서는 랜덤하게 뒤섞인다. 이는 과적합(overfitting)을 막기 위함이다.

### Limitations of Current Systems
* 메타데이터 처리 측면
  - Lustre는 DNE (i.e., Distributed Namespace)라는 기능을 통해 서브-디렉토리 또는 디렉토리의 구조를 MDTs (Metadata Targets)에 분산시켜 메타데이터 처리 속도를 높인다.
  - 그러나, (1) 서브-디렉토리는 각각 하나의 MDT에만 매핑되므로, 한 디렉토리 내 파일이 매우 많은 경우 MDT에 가해지는 부하가 매우 커지고, (2) 디렉토리 구조를 분산시키는 경우엔 한 디렉토리의 메타데이터가 여러 MDT에 나뉘어 저장되기 때문에, 메타데이터를 가져오는 데에 MDT 순회가 다수 발생한다는 문제가 있다.
* 캐싱 시스템 측면
  - Global in-memory caching system(e.g., Memcached)을 통해 클러스터 내에서의 파일 접근 속도를 향상시킬 수 있다.
  - 그렇지만, 시스템을 구성하는 한 노드에 문제가 생기면 전체 작업이 중단되고, 데이터 세트가 캐시에 다 저장되지 않는 경우 스레싱이 발생할 수 있다는 문제점이 있다.


## RELATED WORK
* **메타데이터 관리 최적화**
  - TableFS, IndexFS, DeltaFS: Log-structured merge tree (LSM-tree) 구조를 활용 
  - GiraffaFS: 메타데이터를 고성능 분산 데이터베이스인 HBase에 저장
  - HopsFS: HDFS의 단일 노드 메타데이터 서비스를 분산 메타데이터 서비스로 변경
  - LocoFS: 디렉토리 컨텐츠와 구조를 분리하여 서로 다른 key-value database에 저장
  - Lustre, GPFS, Ceph: 파일 네임스페이스별로 캐싱 시스템 보유, 파일 탐색 및 디렉토리 순회 속도 향상
  - Google GFS: 단일 마스터 노드에서 메타데이터를 메인 메모리에 저장
  - **DIESEL은 분산된 key-value store를 이용하며, 메타데이터 스냅샷 기능을 제공한다.**
* **딥러닝을 위한 분산 스토리지/캐싱**
  - FanStore: MPI-based 런타임 스토리지 시스템으로, 데이터 세트를 청크로 분할하여 여러 노드에 분산시켜 저장한다. (Chunk-wise shuffling 미지원)
  - DeepIO: In-memory 스토리지 시스템으로, memory-based shuffling, entropy-aware opportunistic ordering techniques가 적용되었다. (Transparent하지 않음)
  - DLFS: DeepIO를 NVMe-oF 장치로 확장시킨 시스템이다.
  - Quiver: 안전한 bash-based 파일 탐색을 채택한 캐싱 시스템으로, co-ordinated eviction, benefit-aware techniques가 적용되었다. (Caching layer에서만 동작하고, storage layer를 고려하지 않음)
  - **DIESEL은 캐싱과 스토리지 단계를 모두 아울러 설계된 시스템이다.**

## DIESEL DESIGN
![Fig 2](../../assets/images/2023-08-01-DIESEL/Fig2.png)  

* 다양한 최적화 방법이 적용되었다. (메타데이터 관리, 분산 캐싱, 데이터 셔플)
* **Request executor**는 작은 파일 요청을 정렬하고, 청크 단위의 연산으로 병합한다.
* 두 가지 파일 접근 방법을 제공한다.
  - libDIESEL 라이브러리를 통해 파일 접근
  - FUSE를 통해 DIESEL을 마운트하여 사용 (libDIESEL로 구현됨)
* **libDIESEL 라이브러리**에는 두 개의 메인 구성 요소가 포함되어 있다.
  - **메타데이터 캐시와 인터프리터**: 로컬 디스크에서 메타데이터 스냅샷을 가져오고 해석한다.
  - **Task-grained 데이터 세트 캐시**: 실행 중 학습 데이터 세트를 캐시한다.
* **DLCMD**는 CLI로, DIESEL에 저장된 데이터 세트를 관리할 때 사용한다.
* **DIESEL 서버**는 기존 스토리지와 key-value 데이터베이스 관리자가 배포할 수 있다.
* **메타데이터 스냅샷**은 서버로부터 자동으로 다운로드하고, 학습 시 메모리에 로드하여 사용한다.

![Fig 3](../../assets/images/2023-08-01-DIESEL/Fig3.png)  
* Figure 3은 DIESEL의 write flow를 보여준다.
1. 사용자가 libDIESEL 또는 DLCMD를 이용해 로컬 디스크의 파일을 DIESEL에 저장한다.
2. 클라이언트 단에서 파일을 묶어서 청크 단위로 만든다. 메타데이터는 청크의 앞부분에 저장된다.
3. DIESEL 서버는 청크를 받아, 메타데이터를 추출한다. 메타데이터는 key-value 쌍으로 만들어 key-value 데이터베이스에 저장하고, 데이터 청크는 스토리지 시스템에 저장한다.

![Fig 4](../../assets/images/2023-08-01-DIESEL/Fig4.png)  
* Figure 4는 DIESEL의 read flow를 보여준다.
1. 사용자는 libDIESEL 또는 FUSE를 통해 읽기 요청을 보낸다.
2. 클라이언트 단에서 task-grained distributed cache가 활성화되어 있다면, 해당 요청은 연관된 피어(peer)로 전송된다.
3. task-grained distributed cache가 비활성화되어 있거나, 캐시 miss가 발생하면 해당 요청은 서버로 전송된다.
4. 서버의 캐시가 활성화되어 있고, 요청에 해당하는 데이터 청크가 캐시되어 있다면, 빠른 오브젝트 스토리지인 SSD-based 스토리지로 요청을 보낸다.
5. 서버의 캐시가 비활성화되어 있거나 캐시 miss가 발생하면, 해당 요청은 느린 오브젝트 스토리지인 HDD-based 스토리지로 전송되고, 서버는 해당 데이터 청크를 빠른 오브젝트 스토리지에 캐시한다.

### Data Layout and Metadata Organization
![Fig 5](../../assets/images/2023-08-01-DIESEL/Fig5.png)
* Figure 5는 데이터 청크의 레이아웃과 메타데이터 설계를 보여준다.
* 데이터 청크는 self-contained한 특성이 있어서, DIESEL 서버는 데이터 청크로부터 모든 key-value 쌍을 복구할 수 있다.
* **메타데이터 처리를 메타데이터 스토리지로부터 분리**
  - 데이터의 직렬화/역직렬화는 key-value 데이터베이스가 아닌 DIESEL 서버에서 수행되며, 파일 시스템 연산 또한 DIESEL 서버가 key-value 연산으로 변환한다.
* **Fault-recovery**
  - 최근에 저장한 key-value 쌍이 소실되는 경우 (node failure) 특정 타임스탬프의 데이터 청크를 스캔한다.
  - 모든 in-memory key-value 쌍이 소실되는 경우 (data center power failure 등) 모든 데이터 청크를 저장한 순서대로 스캔한다. 타임스탬프를 이용해 청크 ID를 생성하므로, 저장한 순서대로 정렬이 가능하다.
* **메타데이터 스냅샷**
  - 메타데이터 데이터베이스에 가해지는 부하를 줄이기 위해, 클라이언트의 로컬 디스크에 메타데이터 스냅샷을 저장한다.
  - 메타데이터 스냅샷에는 데이터 세트 업데이트 타임스탬프 (최신 버전인지 확인하는 용도), 청크 ID 리스트, 파일의 메타데이터(청크 ID, 오프셋, 길이, 전체 이름)가 존재한다.
  - 공유 스토리지에 저장하면, 클라이언트마다 스냅샷을 저장할 필요가 없다.

### Task-Grained Distributed Cache
* Global in-memory 캐싱 시스템이 존재하지만, DLT는 샘플을 무작위 순서로 읽기 때문에 node failure로 인한 성능 저하가 심각하다.
  - Global in-memory caching system: 클러스터 내의 모든 노드의 메모리를 하나의 큰 메모리처럼 사용. 더 많은 데이터를 캐싱할 수 있다.
![Fig 6](../../assets/images/2023-08-01-DIESEL/Fig6.png) 
* Figure 6는 캐시 적중률에 따른 읽기 속도를 나타낸 것이다. 적중률이 낮아지면, 파일 접근 속도는 더 큰 폭으로 떨어진다.
* *Task-grained distributed cache*는 데이터 세트를 여러 DTL 노드에 걸쳐 캐싱한다.
* 일부 노드에 문제가 생기면 (캐시 miss를 의미하는 듯), 해당 노드에 영향을 받는 DTL 작업만 중단된다.
* 청크 단위로 관리하므로, 데이터 세트를 빠르게 캐싱할 수 있다.
![Fig 7](../../assets/images/2023-08-01-DIESEL/Fig7.png) 
* Figure 7은 task-grained distributed cache의 동작을 보여준다.
  - 사용자 애플리케이션은 여러 개의 I/O worker를 실행한다. 각각의 worker는 DIESEL 클라이언트를 생성한다.
  - DIESEL 클라이언트에는 랭크가 매겨져 있다.
  - 랭크 0 클라이언트가 마스터가 된다. 마스터 클라이언트는 다른 클라이언트에 데이터를 분배한다.
  - 모든 클라이언트는 마스터 클라이언트에 연결되어 있어, 캐시에서 데이터를 탐색하는 데 걸리는 hop은 항상 1이다. (DeltaFS에서 사용하는 multi-hop routing 메소드보다 효율적)
* **Oneshot cache policy**: DIESEL 서버에 등록하는 즉시 데이터 세트를 로컬 스토리지로 다운받는다. 사용자가 학습시킬 모델을 로드하면, DIESEL 클라이언트가 데이터 세트를 백그라운드에서 캐싱하여 첫 번째 에포크에 걸리는 시간이 줄어든다. (보통 첫 번째 에포크를 cache warming 단계로 치는데, 여기서는 학습 시작 전부터 hot cache를 만드는 것)
  - On-demand cache policy(캐시 miss가 발생하면 필요한 데이터 청크를 가져온다.)도 지원한다.

### Shuffle in Memory-Constrained Scenario
* DLT 작업에서 모델의 과적합을 방지하기 위해 데이터 세트 학습 순서를 매번 섞는데, 이러한 I/O 특성은 성능을 크게 저하시킨다. (특히, 데이터 세트를 100% 캐싱할 수 없을 때)
* DIESEL은 한 번에 많은 데이터를 읽으면서, 매번 무작위 순서를 유지하는 *chunk-wise shuffle* 방법을 제시한다.

![Fig 8](../../assets/images/2023-08-01-DIESEL/Fig8.png) 
* Figure 8은 chunk-wise shuffling을 보여준다.
  1. 청크를 무작위로 섞는다.
  2. 청크를 N개의 그룹으로 나눈다. (N은 사용자가 지정한 그룹 크기)
  3. DIESEL은 그룹에 포함된 파일 정보를 추출하여, 해당 그룹 내의 파일들을 무작위로 섞는다.
  4. 클라이언트에서 파일을 읽으면, 해당 파일이 속한 청크 전체를 캐싱한다.


## IMPLEMENTATION
* DIESEL 서버는 Go로, 클라이언트는 C++로 구현했다.
* 데이터 스토리지로 Ceph를 비롯한 POSIX-based 분산 스토리지 시스템을 지원한다.
* 메타데이터 스토리지로 Redis를 비롯한 key-value 데이터베이스를 지원한다.
* 서버와 클라이언트 사이 RPC 레이어로 Apach Thrift를 사용했다.

## EVALUATION
### Experimental Setup
* **노드 구성**
  - 6대의 스토리지 노드(3.8TB NVMe SSDs)를 사용했다.
  - 10대의 테스트 노드에서 DIESEL과 학습 애플리케이션을 실행했다.
  - 테스트 노드 중 4대의 노드(Tesla V100 SXM2 32GB * 8)에서 DLT 워크로드를 실행했다. (한 노드에 8개의 GPU 카드를 추가한 이유는, task-grained distributed cache를 테스트하기 위함이다.)
  - 모든 노드는 100Gbps InfiniBand 네트워크로 연결했다.
* **소프트웨어**
  - Underlying 스토리지 시스템으로 Lustre를 사용하여, 데이터 청크는 multi-level 디렉토리에 저장되며, DNE 기능으로 메타데이터 오버헤드가 줄어든다.
  - 메타데이터 스토리지로 Redis를 사용했다.
  - MPI 도구를 제작하여 여러 노드에서 동시에 발생하는 DTL 학습 I/O 패턴을 재현했다.
  - *Twemproxy*를 이용하여 10대의 테스트 노드로 Memcached 클러스터를 구축했다. (한 노드에는 Memcached 서버를 실행하는 16개의 스레드와, 8개의 Twemproxy 인스턴스가 존재)
  - Memcached 클러스터와 상호작용을 하는 데 *libMemcached* 라이브러리를 사용했다.
* **워크로드**
  - PyTorch에서 제공하는 공식 예제를 그대로 사용했다.

### Performance on File Writing
![Fig 9](../../assets/images/2023-08-01-DIESEL/Fig9.png) 
* **Figure 9**는 1초에 쓸 수 있는 파일의 개수를 클러스터 종류와 파일 크기를 바꾸어서 비교한 결과이다.
* 4개의 노드에 파일을 쓰는 연산이며, 64개의 MPI 프로세스를 사용했다.
* DIESEL는 같은 시간동안 Memcached, Lustre보다 더 많은 파일을 쓸 수 있으며, 이러한 성능 차이는 파일 크기가 작은 경우 더욱 두드러진다.
  - DIESEL은 파일을 청크 단위로 버퍼링하여 연산을 수행하고, 이로서 쓰기 연산 횟수를 줄여 성능을 향상시킨다. 
  - 파일 하나의 크기가 커지면 연산 횟수가 비교적 덜 줄어들기 때문에 성능 향상 정도도 작다. (청크 크기가 4MB라면, 4KB 파일을 쓰는 횟수는 1/1000으로, 128KB 파일을 쓰는 횟수는 약 1/32로 줄어든다.)
  - Lustre는 범용 파일 시스템으로, 복잡한 파일 록과 메타데이터 연산이 있어 성능이 떨어진다.
  - Memcached 클라이언트에는 배치 모드가 없으며, 복수의 쓰기 요청을 병합하여 보내는 기능은 있으나, 모든 쓰기 요청이 원격 프로시저를 호출한다.

### Efficiency of Metadata Access and Snapshot
![Fig 10](../../assets/images/2023-08-01-DIESEL/Fig10.png) 
* **Figure 10**은 메타데이터 액세스 성능을 다양한 환경에서 측정한 결과이다.
* 각 노드는 16개의 클라이언트 스레드를 실행한다.
* **Figure 10 (a)**는 DIESEL 클라이언트 노드와 서버 수에 따라 1초에 처리할 수 있는 쿼리 수의 변화를 보여준다.
  - *memtier_benchmark*로 테스트했다.
  - DIESEL 서버가 많을수록 클러스터의 scalability가 높다.
* **Figure 10 (b)**는 클라이언트 노드 수와 클러스터 종류에 따른 메타데이터 탐색 연산의 성능 차이를 보여준다.
  - DIESEL 클라이언트는 각각 로컬 스토리지에 메타데이터 스냅샷이 존재하므로, 클라이언트가 늘어남에 따라 QSP도 상승한다.
  - Lustre 클러스터는 모든 요청을 MDS 서버에서 처리하므로, 클라이언트가 늘어나도 QPS는 변화하지 않는다.
* **Figure 10 (c)**는 메타데이터 연산 종류와 파일 시스템 종류에 따른 소요 시간 차이를 보여준다.
  - `ls -R`은 파일 이름만 가져오고, `ls -lR`은 파일 이름과 크기를 가져온다.
  - `ls -lR`에 걸리는 시간은 Lustre가 제일 길다. 그 이유는 Lustre 시스템에서 파일 이름은 MDS에 저장되지만, 파일 크기는 OSS (object-storage service)에 저장되어, 한 파일의 크기를 가져올 때에도 원격 프로시저를 여러 차례 호출하기 때문이다. (부연: 이 탓에 MDS에 추정 파일 크기를 저장하는 기능도 제공한다.)
  - DIESEL은 로컬 스냅샷을 메인 메모리의 해시맵으로 로딩하여 O(1)만에 메타데이터를 가져올 수 있다.

### Efficiency of Task-Grained Distributed Cache
![Fig 11](../../assets/images/2023-08-01-DIESEL/Fig11.png) 
* **Figure 11**은 task-grained distributed cache와 Memcached의 성능을 비교한 것이다.
* **Figure 11 (a)**는 클러스터 종류와 노드 개수에 따라 초당 읽을 수 있는 파일 개수의 차이를 보여준다.
  - 한 파일의 크기는 4KB이다.
  - Lustre는 메인 메모리를 활용하지 않아 다른 클러스터에 비해 성능이 매우 낮다.
  - Memcached는 데이터 세트를 여러 Memcached 서버에 나누어 캐싱하기 때문에, 모든 노드가 모든 서버에 연결되어 있어야 한다. Memcached의 성능이 DIESEL보다 낮은 것은 그로 인한 네트워크 오버헤드가 원인이다.
  - DIESEL-FUSE의 오버헤드는 커널에서의 문맥 교환(context switch)이다. 이는 FUSE 인터페이스의 한계이다.

### Efficiency of Chunk-Wise Shuffle Method
![Fig 12](../../assets/images/2023-08-01-DIESEL/Fig12.png) 
* **Figure 12**는 파일 크기와 클러스터 종류에 따른 대역폭을 비교한 것이다.
* 10개의 노드가 각각 16개의 스레드를 실행한다.
* Chunk-wise shuffling은 기존 방식 대비 효율적이며, 이는 파일 크기가 작은 경우 더욱 그렇다.

![Fig 13](../../assets/images/2023-08-01-DIESEL/Fig13.png) 
* **Figure 13**은 모델의 정확도와 컨버전스 속도를 나타낸 것이다.
* ResNet-50 (ImageNet-1K 데이터 세트), ResNet-18 (CIFAR-10 데이터 세트)로 실험했다.
* Top-1 정확도는 모델이 정확한 답변을 한 비율을 나타내고, top-5 정확도는 모델이 5회 내에 옳은 답변을 한 비율을 나타낸다.
* Chunk-wise shuffling은 모델의 정확도나 컨버전스 속도에 영향을 미치지 않는 것으로 드러났다.

### Evaluation in DLT Tasks
![Fig 14](../../assets/images/2023-08-01-DIESEL/Fig14.png) 
* Figure 14는 첫 10회의 에포크 동안 평균 데이터 접근 속도를 나타낸 것이다.
* 모델에 관계없이, DIESEL-FUSE의 데이터 접근 시간은 Lustre의 절반 수준으로 낮았다.

![Fig 15](../../assets/images/2023-08-01-DIESEL/Fig15.png) 
* Figure 15는 클러스터 종류와 모델 종류에 따른 전체 학습 시간 차이를 비교한 것이다.
* 전체 학습 시간은 Lustre에서 약 37-66시간, DIESEL에서 약 29-57시간이었다.
  - 다른 DLT 프레임워크(e.g., TensorFlow)에 적용할 때에도, 유사한 수준의 성능 향상이 가능할 것이다.

## CONCLUSION
* 본 논문에서는 DLT 작업을 위한 스토리지 및 캐싱 시스템인 DIESEL을 설계하고 구현했다.
* DIESEL은 스토리지-캐싱을 상호 고려하여 설계되었다.
* 메타데이터 스냅샷 기능을 통해, 메타데이터 스토리지와 메타데이터 처리를 분리했다.
* Task-grained distributed cache 개념을 도입해, 개별 DTL 작업에 발생한 문제가 학습 전체에 미치는 영향을 최소화했다.
* Chunk-wise shuffling을 고안하여, 청크 단위 I/O면서 동시에 개별 파일을 무작위 순서로 읽을 수 있도록 만들었다. (과적합 방지를 위해 필수적)
* DIESEL 시스템을 이용하여 DTL 작업을 하는 경우, 실제로 전체 학습 시간이 크게 줄어듦을 확인할 수 있었다.