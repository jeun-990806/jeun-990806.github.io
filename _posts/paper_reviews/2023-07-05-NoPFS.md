---
title: Clairvoyant Prefetching for Distributed Machine Learning I/O
author: Jieun
date: 2023-07-05
layout: post
category: paper_reviews
---

* 요약: 

---

## 1. Introduction
 * DNNs 학습 Workloads는 수퍼컴퓨터에 있어 아주 중요하지만, 학습 비용이 매우 크다.
 * DL Framework의 측면에서, DNN 학습은 세 가지로 이루어진다; DNN을 수행하기 위한 계산(computation), 통신(commnication), 노드 사이 업데이트 동기화(synchronization)
 * 이전의 연구들은 계산과 통신을 최적화하는 데 중점을 두었으나, 점점 I/O가 Bottleneck이 되어가고 있다.
    - We find taht when training ResNet-50 on ImageNet at scale, up to 85% of runtime is I/O overhead, and we observe similar trends in other datasets.
 * Stochastic gradient descent(SDG)를 최적화하는 것은 어려운데, 왜냐하면 데이터 샘플들을 무작위로 읽기 때문이다.
    - 특히 분산 학습 시, 공유 파일 시스템에서 데이터가 경합(contention) 상태일 때 더욱 문제가 된다.


## 2. Background
### 2.1. Data Sharding
### 2.2. Machin Learning I/O Frameworks

## 3. I/O Access Patterns
### 3.1. Distribution of Accesses

## 4. Performance Modeling

## 5. NoPFS
### 5.1. Design
### 5.2. Implementation

## 6. Performance Simulator
### 6.1. Simulation Results
### 6.2. Environment Evaluation

## 7. Evaluation
### 7.1. I/O Performance
### 7.2. End-to-end Training

## 8. Related Work

## 9. Conclusion
